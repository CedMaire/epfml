{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_loader import load_data, DATA_PATH_TEST, DATA_PATH_TRAIN, DATA_PATH_SAMPLE_SUBMISSION_TEST\n",
    "import logistic_regression\n",
    "import gradient_descent\n",
    "import matplotlib.pyplot as plt\n",
    "from label_predictor import predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids_train = load_data(DATA_PATH_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, number_of_subset, seed):\n",
    "    rows_num = len(y)\n",
    "    inter = int(rows_num / number_of_subset)\n",
    "    #set the random seed\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(rows_num)\n",
    "    subset_indices = []\n",
    "    for i in range(number_of_subset):\n",
    "        subset_indices.append(indices[i * inter: (i + 1) * inter])\n",
    "    return np.array(subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, tx, k_indices, k, gamma, alpha):\n",
    "    #get the indices of the subsets\n",
    "    test_set = k_indices[k]\n",
    "    train_set = k_indices[~(np.arange(k_indices.shape[0]) == k)].reshape(-1)\n",
    "    \n",
    "    #get the subsets\n",
    "    tx_train = tx[train_set]\n",
    "    y_train = y[train_set]\n",
    "    tx_test = tx[test_set]\n",
    "    y_test = y[test_set]\n",
    "\n",
    "    \n",
    "    w, loss_train = logistic_regression.regularized_logistic_regression_gradient_descent(y_train, tx_train, gamma, 50,alpha)\n",
    "    loss_test = logistic_regression.calculate_loss(y_test, tx_test, w)\n",
    "    y_pred = predict_labels(w, tx_test)\n",
    "    counter = 0\n",
    "    #change the -1 to 0 to match y_test\n",
    "    y_pred = [0 if x==-1 else x for x in y_pred]\n",
    "    for i in range(y_test.shape[0]):\n",
    "        if y_pred[i] == y_test[i]:\n",
    "            counter += 1\n",
    "    percent = 100*counter/y_test.shape[0]\n",
    "#     print(percent)\n",
    "    return loss_train, loss_test, w, percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(tx, degree):\n",
    "    for idx, x in enumerate(tx.T):\n",
    "        if idx == 0:\n",
    "            arr_out = build_poly_one_column(x, degree)\n",
    "        else:\n",
    "            arr_out = np.c_[arr_out, build_poly_one_column(x, degree)]\n",
    "    return arr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_one_column(x, degree):\n",
    "    arr = np.zeros((x.shape[0], degree+1))\n",
    "    for degre in range(degree+1):\n",
    "        arr[:,degre] = np.power(x, degre)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\benja\\git\\MLProjet\\epfml\\src\\logistic_regression.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_2 = np.matmul((1 - y).T,(np.log(1 - sigma)))\n",
      "D:\\Users\\benja\\git\\MLProjet\\epfml\\src\\logistic_regression.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_1 = -np.matmul(y.T,(np.log(sigma)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.63629402756509\n",
      "89.1931216931217\n",
      "89.46476964769649\n",
      "92.95392953929539\n",
      "73.89548436060065\n",
      "63.62817947985138\n",
      "66.13950742240218\n",
      "65.77547863082576\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_test():    \n",
    "#     tx_train = np.delete(tx, [5, 12, 15, 18, 19, 20, 21, 23, 25, 27, 28, 29, 30], axis=1)\n",
    "    for y_t, tx_t, id_test in zip(y, tx, ids_train):\n",
    "        # split data in k fold\n",
    "        number_of_subset = 4\n",
    "        subset_indices = build_k_indices(y_t, number_of_subset, 12) #last number is the seed  \n",
    "        percents = []\n",
    "        tx_train = build_poly(tx_t, 2)\n",
    "        for i in range(number_of_subset):\n",
    "            _,_,_, percent = cross_validation(y_t, tx_train, subset_indices, i, 0.5,0.5)\n",
    "            #print(percent)\n",
    "            percents.append(percent)\n",
    "        print(np.mean(percents))\n",
    "\n",
    "cross_validation_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\benja\\git\\MLProjet\\epfml\\src\\logistic_regression.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_2 = np.matmul((1 - y).T,(np.log(1 - sigma)))\n",
      "D:\\Users\\benja\\git\\MLProjet\\epfml\\src\\logistic_regression.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_1 = -np.matmul(y.T,(np.log(sigma)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestRatio: 94.63629402756509\n",
      "Set  0\n",
      "bestW  [-0.14526555 -0.14526555 -0.14526555 -0.14526555  0.14302304  0.16658398\n",
      " -0.14526555  0.01919022 -0.66265613 -0.14526555 -0.00682256 -0.12060465\n",
      " -0.14526555 -0.04619443 -0.26854505 -0.14526555 -0.00682284 -0.12060495\n",
      " -0.14526555  0.42408973  0.15267577 -0.14526555 -0.34200832 -0.13782914\n",
      " -0.14526555 -0.07503385 -0.01331175 -0.14526555  0.48325103 -0.16683199\n",
      " -0.14526555  0.04058689 -0.01780975 -0.14526555 -0.01773919 -0.12911693\n",
      " -0.14526555  0.18004038 -0.26865787 -0.14526555  0.04508566 -0.17553439\n",
      " -0.14526555  0.0151609  -0.09603653 -0.14526555  0.17154488  0.24178848\n",
      " -0.14526555  0.0283974  -0.10461213 -0.14526555 -0.00823478 -0.15668107]\n",
      "bestG  0.5\n",
      "bestA  0.5\n",
      "bestD  2\n",
      "bestRatio: 91.17724867724867\n",
      "Set  1\n",
      "bestW  [-0.079317   -0.079317   -0.079317   -0.079317    0.11677488  0.00608304\n",
      " -0.079317    0.03323589 -0.26166997 -0.079317   -0.0749151   0.02893454\n",
      " -0.079317   -0.03111449 -0.16121121 -0.079317    0.13082433  0.06180063\n",
      " -0.079317    0.2166594  -0.03985218 -0.079317   -0.14576645 -0.04505354\n",
      " -0.079317   -0.09421132 -0.00209528 -0.079317    0.30000334  0.03037206\n",
      " -0.079317    0.02762712 -0.05146138 -0.079317    0.02886805 -0.11230243\n",
      " -0.079317    0.09286483  0.02588108 -0.079317    0.01847112 -0.08175318\n",
      " -0.079317    0.01205196 -0.06087844 -0.079317    0.11331355  0.03129314\n",
      " -0.079317   -0.01109104 -0.05746698 -0.079317   -0.01972868 -0.05581636\n",
      " -0.079317    0.08228251 -0.05159735 -0.079317   -0.02025004  0.03684354\n",
      " -0.079317   -0.02061571 -0.03243027 -0.079317    0.0822826  -0.05159724]\n",
      "bestG  0.1\n",
      "bestA  0.1\n",
      "bestD  2\n",
      "bestRatio: 90.27777777777777\n",
      "Set  2\n",
      "bestW  [-0.06526036 -0.06526036 -0.06526036 -0.06526036  0.08073688  0.06508496\n",
      " -0.06526036 -0.05101031 -0.92618776 -0.06526036 -0.16551231  0.16293444\n",
      " -0.06526036  0.12852901 -0.10820473 -0.06526036  0.26890657  0.90710466\n",
      " -0.06526036 -0.00405041  0.01977181 -0.06526036 -0.23057163 -0.50561028\n",
      " -0.06526036  0.91585964  0.19140416 -0.06526036  0.43346944 -0.34090047\n",
      " -0.06526036 -0.37039213 -0.23813427 -0.06526036 -0.24551859  0.27567726\n",
      " -0.06526036  0.23269457  0.37110907 -0.06526036  0.80539104  0.06056157\n",
      " -0.06526036 -0.00921537 -0.42364962 -0.06526036  0.00713556 -0.23405159\n",
      " -0.06526036  0.30464104  0.09149586 -0.06526036  0.01367257 -0.23270618\n",
      " -0.06526036 -0.08182683 -0.07133459 -0.06526036 -0.05623195  0.11651312\n",
      " -0.06526036 -0.05143587  0.08303223 -0.06526036 -0.69621876 -0.0276273\n",
      " -0.06526036  0.02774356 -0.22391995 -0.06526036 -0.19784256  0.15826279\n",
      " -0.06526036 -0.05577444 -0.16176239 -0.06526036  0.30308781 -0.06185247\n",
      " -0.06526036  0.0603926  -0.06577363 -0.06526036  0.29206139 -0.12050038\n",
      " -0.06526036  0.1275936  -0.19404542]\n",
      "bestG  0.5\n",
      "bestA  0.1\n",
      "bestD  2\n",
      "bestRatio: 93.08943089430893\n",
      "Set  3\n",
      "bestW  [-0.26435583 -0.26435583 -0.26435583 -0.26435583  0.30943033 -0.40132975\n",
      " -0.26435583 -0.25597436 -1.27925232 -0.26435583  0.05590195  0.29461215\n",
      " -0.26435583 -0.11983326  0.16875962 -0.26435583  0.1115596  -0.009905\n",
      " -0.26435583 -0.14528545 -0.05805912 -0.26435583 -0.15658557 -0.08768894\n",
      " -0.26435583  0.32856349 -0.1111829  -0.26435583  0.0642096   0.26132039\n",
      " -0.26435583 -0.19011208 -0.27502234 -0.26435583 -0.28144617 -0.21643977\n",
      " -0.26435583  0.11693913  0.18589037 -0.26435583  0.16949185  0.1377994\n",
      " -0.26435583 -0.49284852 -0.36564945 -0.26435583  0.13523902 -0.305024\n",
      " -0.26435583  0.32308737  0.1413936  -0.26435583 -0.06850949 -0.00681178\n",
      " -0.26435583 -0.37891152 -0.09167572 -0.26435583  0.0287239  -0.34343972\n",
      " -0.26435583  0.10107885 -0.23793448 -0.26435583 -0.457186    0.26262111\n",
      " -0.26435583  0.03266996 -0.37263466 -0.26435583 -0.08971664 -0.34713733\n",
      " -0.26435583 -0.05354753 -0.12312289 -0.26435583  0.0556508  -0.15615375\n",
      " -0.26435583 -0.06037024 -0.39380572 -0.26435583 -0.02505781 -0.1088424\n",
      " -0.26435583 -0.07317756 -0.88615565]\n",
      "bestG  0.6\n",
      "bestA  0.1\n",
      "bestD  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\benja\\git\\MLProjet\\epfml\\src\\logistic_regression.py:13: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return loss_1 + loss_2 / len(sigma)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bestRatio: 79.70401691331924\n",
      "Set  4\n",
      "bestW  [-0.02220702 -0.02220702 -0.02220702 -0.02220702 -0.47787745  0.10079395\n",
      " -0.02220702 -0.05455501 -0.3491103  -0.02220702 -0.07005964  0.06729159\n",
      " -0.02220702  0.42913035 -0.35949092 -0.02220702 -0.07005959  0.06729169\n",
      " -0.02220702  0.1983064  -0.05207411 -0.02220702 -0.44129686  0.07112309\n",
      " -0.02220702  0.00098036  0.04009628 -0.02220702  0.41282302 -0.02283691\n",
      " -0.02220702  0.00330699 -0.10101111 -0.02220702  0.00354647 -0.00864553\n",
      " -0.02220702 -0.10740618  0.03496943 -0.02220702  0.02230704 -0.21011447\n",
      " -0.02220702  0.00417427 -0.02022429 -0.02220702 -0.33998975  0.08816527\n",
      " -0.02220702 -0.03234727  0.00352862 -0.02220702  0.1237931  -0.00608006]\n",
      "bestG  0.2\n",
      "bestA  0.8\n",
      "bestD  2\n",
      "bestRatio: 75.27579308373822\n",
      "Set  5\n",
      "bestW  [ 2.85635272e-03  2.85635272e-03  2.85635272e-03  2.85635272e-03\n",
      " -3.63831617e-01 -6.82291746e-02  2.85635272e-03  1.89748393e-01\n",
      " -3.67182458e-01  2.85635272e-03  9.97153883e-02  1.01679496e-01\n",
      "  2.85635272e-03  3.25466273e-01 -4.10444802e-01  2.85635272e-03\n",
      "  5.68396156e-02 -2.28148289e-04  2.85635272e-03  1.88345270e-01\n",
      " -1.15821789e-02  2.85635272e-03 -2.16522580e-01 -9.21892599e-03\n",
      "  2.85635272e-03  2.65141457e-01  2.62010119e-02  2.85635272e-03\n",
      "  3.23864064e-01 -3.45604043e-04  2.85635272e-03 -5.25431724e-03\n",
      " -1.13788807e-01  2.85635272e-03 -1.74488922e-02  4.02604988e-03\n",
      "  2.85635272e-03  4.79144614e-02  2.52355089e-02  2.85635272e-03\n",
      " -5.27684719e-03 -1.73696912e-01  2.85635272e-03 -8.46705936e-03\n",
      "  1.28926224e-02  2.85635272e-03  7.77839862e-02  2.29021135e-02\n",
      "  2.85635272e-03  1.13560005e-02  4.35281444e-03  2.85635272e-03\n",
      "  9.59292569e-02 -5.86911062e-02  2.85635272e-03  1.01546099e-01\n",
      "  3.59043952e-02  2.85635272e-03 -4.18881469e-03  3.65020036e-01\n",
      "  2.85635272e-03 -4.65237557e-03  6.02948147e-03  2.85635272e-03\n",
      "  1.01546139e-01  3.59043613e-02]\n",
      "bestG  0.1\n",
      "bestA  0.1\n",
      "bestD  2\n",
      "bestRatio: 75.74224021592443\n",
      "Set  6\n",
      "bestW  [ 0.0086983   0.0086983   0.0086983   0.0086983  -0.21213497 -0.13890679\n",
      "  0.0086983   0.36297524 -0.42417037  0.0086983   0.17924272  0.08013063\n",
      "  0.0086983   0.15219336  0.13404015  0.0086983   0.15305692  0.1844012\n",
      "  0.0086983  -0.14180692  0.04563558  0.0086983   0.33299976 -0.32931086\n",
      "  0.0086983  -0.20484607  0.02614214  0.0086983   0.12394335 -0.02929927\n",
      "  0.0086983  -0.15172332 -0.0490051   0.0086983   0.22973109 -0.02017212\n",
      "  0.0086983   0.34207992  0.06995712  0.0086983   0.27820307 -0.00924392\n",
      "  0.0086983  -0.00438665 -0.12922477  0.0086983  -0.00809025 -0.00285213\n",
      "  0.0086983   0.10059097  0.04049733  0.0086983  -0.01376827 -0.15928919\n",
      "  0.0086983   0.00972743  0.00291901  0.0086983   0.18421134  0.02965792\n",
      "  0.0086983   0.01414874  0.00263641  0.0086983  -0.04858918 -0.06066321\n",
      "  0.0086983  -0.00740606 -0.07728092  0.0086983   0.01795688  0.11245318\n",
      "  0.0086983   0.01239495  0.01722527  0.0086983   0.10152001 -0.0355166\n",
      "  0.0086983   0.01058107  0.10630885  0.0086983  -0.01272343 -0.00759297\n",
      "  0.0086983   0.02730312 -0.05112907]\n",
      "bestG  0.1\n",
      "bestA  0.30000000000000004\n",
      "bestD  2\n",
      "bestRatio: 74.07658093212144\n",
      "Set  7\n",
      "bestW  [-6.98662307e-02 -6.98662307e-02 -6.98662307e-02 -6.98662307e-02\n",
      " -1.26782455e+00 -9.40675679e-01 -6.98662307e-02  2.46282174e+00\n",
      " -2.60957185e+00 -6.98662307e-02  1.80841846e+00  7.68963873e-01\n",
      " -6.98662307e-02 -2.60419723e-01  1.52233300e+00 -6.98662307e-02\n",
      " -2.38707066e-01  1.32539827e+00 -6.98662307e-02 -2.14404182e-01\n",
      "  9.84935310e-01 -6.98662307e-02  1.80654293e+00 -2.40964245e+00\n",
      " -6.98662307e-02 -5.10074310e-01 -1.08478524e-01 -6.98662307e-02\n",
      " -3.87907252e-02  3.95067958e-02 -6.98662307e-02 -1.04717636e+00\n",
      " -5.10731781e-01 -6.98662307e-02  1.49595649e+00 -2.53158499e-01\n",
      " -6.98662307e-02  1.06696356e+00  1.12729490e+00 -6.98662307e-02\n",
      "  2.01992904e+00  2.52210752e-02 -6.98662307e-02 -1.82040030e-01\n",
      " -1.17603941e+00 -6.98662307e-02  1.50662120e-01 -6.89033492e-02\n",
      " -6.98662307e-02  7.81021458e-01  3.24616988e-01 -6.98662307e-02\n",
      " -1.44298090e-01 -1.34493727e+00 -6.98662307e-02  8.83277864e-02\n",
      " -1.13887317e-01 -6.98662307e-02  1.55067895e+00  3.73684769e-01\n",
      " -6.98662307e-02  1.36544916e-02 -1.89061988e-01 -6.98662307e-02\n",
      " -2.48597699e-01 -2.41949022e-03 -6.98662307e-02 -5.91146134e-01\n",
      " -5.60621815e-01 -6.98662307e-02  1.42022299e-02  1.20609221e-01\n",
      " -6.98662307e-02 -6.74607847e-02 -1.48892547e-01 -6.98662307e-02\n",
      " -2.53991104e-01 -5.64773048e-01 -6.98662307e-02  1.64946364e-01\n",
      "  1.57242790e-02 -6.98662307e-02  7.38415772e-02 -2.12322965e-01\n",
      " -6.98662307e-02 -7.53392089e-01 -2.25766784e-01]\n",
      "bestG  0.8\n",
      "bestA  0.30000000000000004\n",
      "bestD  2\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_search_param(tx, y):\n",
    "    # split data in k fold\n",
    "    number_of_subset = 4\n",
    "    subset_indices = build_k_indices(y, number_of_subset, 12) #last number is the seed\n",
    "    \n",
    "    #Best parameter\n",
    "    bestW = []\n",
    "    bestRatio = 0\n",
    "    bestG = 0\n",
    "    bestA = 0\n",
    "    bestD = 0\n",
    "    \n",
    "    count = 0\n",
    "    #Test of different degrees\n",
    "    for d in np.arange(2, 4, 1):  \n",
    "        tx_train = build_poly(tx, d)\n",
    "        #Test of different alpha\n",
    "        for a in np.arange(0.1, 1, 0.1):\n",
    "            #Test of different gamma\n",
    "            for g in np.arange(0.1, 1, 0.1):\n",
    "                # define lists to store the ratio of true mapping\n",
    "                ratio = 0\n",
    "                ratios= []\n",
    "                for k in range(number_of_subset):\n",
    "                    _, _, w, ratio = cross_validation(y, tx_train, subset_indices, k, g, a)\n",
    "                    ratios.append(ratio)\n",
    "                if np.mean(ratios) > bestRatio:\n",
    "                    bestW = w\n",
    "                    bestG = g\n",
    "                    bestA = a\n",
    "                    bestD = d\n",
    "                    bestRatio = np.mean(ratios)\n",
    "                count += 1\n",
    "#                 if(count%100 == 0):\n",
    "#                     print(count)\n",
    "#                 print(\"ratio:\", np.mean(ratios))\n",
    "    print(\"bestRatio:\", bestRatio)\n",
    "    return bestW, bestG, bestA, bestD\n",
    "counter = 0\n",
    "for y_t, tx_t, id_test in zip(y, tx, ids_train):\n",
    "    bestW, bestG, bestA, bestD = cross_validation_search_param(tx_t, y_t)\n",
    "    print(\"Set \", counter)\n",
    "    print(\"bestW \", bestW)\n",
    "    print(\"bestG \", bestG)\n",
    "    print(\"bestA \", bestA)\n",
    "    print(\"bestD \", bestD)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
