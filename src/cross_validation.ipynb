{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_loader import load_data, DATA_PATH_TEST, DATA_PATH_TRAIN, DATA_PATH_SAMPLE_SUBMISSION_TEST\n",
    "import logistic_regression\n",
    "import gradient_descent\n",
    "import matplotlib.pyplot as plt\n",
    "from label_predictor import predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, tx, ids_train = load_data(DATA_PATH_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k):\n",
    "    # get k'th subgroup in test, others in train\n",
    "    test_indice = k_indices[k]\n",
    "    train_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    train_indice = train_indice.reshape(-1)\n",
    "    y_test = y[test_indice]\n",
    "    y_train = y[train_indice]\n",
    "    x_test = x[test_indice]\n",
    "    x_train = x[train_indice]\n",
    "    \n",
    "    w, loss_train = logistic_regression.regularized_logistic_regression_gradient_descent(y_train, x_train, 0.1, 1000,0.3)\n",
    "    loss_test = logistic_regression.calculate_loss(y_test, x_test, w)\n",
    "    y_pred = predict_labels(w, x_test)\n",
    "    counter = 0\n",
    "    y_pred = [0 if x==-1 else x for x in y_pred]\n",
    "    for x in range(y_test.shape[0]):\n",
    "        if y_pred[x] == y_test[x]:\n",
    "            counter += 1\n",
    "    print(100*counter/y_test.shape[0])\n",
    "    return loss_train, loss_test,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly(tx, degree):\n",
    "    for idx, x in enumerate(tx.T):\n",
    "        if idx == 0:\n",
    "            arr_out = build_poly_one_column(x, degree)\n",
    "        else:\n",
    "            arr_out = np.c_[arr_out, build_poly_one_column(x, degree)]\n",
    "    return arr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_one_column(x, degree):\n",
    "    arr = np.zeros((x.shape[0], degree+1))\n",
    "    for degre in range(degree+1):\n",
    "        arr[:,degre] = np.power(x, degre)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\benja\\git\\MLProjet\\epfml\\src\\logistic_regression.py:8: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(y.T.dot(np.log(sigma)) + (1 - y).T.dot(np.log(1 - sigma))) / len(sigma)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.0\n",
      "the loss of the training set is:  nan\n",
      "the loss of the test set is:  nan\n",
      "79.3584\n",
      "the loss of the training set is:  nan\n",
      "the loss of the test set is:  nan\n",
      "77.5136\n",
      "the loss of the training set is:  nan\n",
      "the loss of the test set is:  nan\n",
      "78.8272\n",
      "the loss of the training set is:  nan\n",
      "the loss of the test set is:  nan\n"
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 12\n",
    "    k_fold = 4\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    # cross validation\n",
    "    loss_tr_tmp = []\n",
    "    loss_te_tmp = []\n",
    "    tx_train = np.delete(tx, [5, 12, 15, 18, 19, 20, 21, 23, 25, 27, 28, 29, 30], axis=1)\n",
    "    tx_train = build_poly(tx_train, 3)\n",
    "    for k in range(k_fold):\n",
    "        loss_tr, loss_te,_ = cross_validation(y, tx_train, k_indices, k)\n",
    "        loss_tr_tmp.append(loss_tr)\n",
    "        loss_te_tmp.append(loss_te)\n",
    "        print(\"the loss of the training set is: \", np.mean(loss_tr_tmp))\n",
    "        print(\"the loss of the test set is: \", np.mean(loss_te_tmp))\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
